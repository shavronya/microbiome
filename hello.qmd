---
title: "Hello, Quarto"
format: html
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r}
if (require(litsearchr)) remotes::install_github("elizagrames/litsearchr", ref = "main")
packages <- c("easyPubMed", "litsearchr", "stopwords", "igraph", "ggplot2", "ggrepel", "ggraph")
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
lapply(packages, library, character.only = TRUE)
```

```{r}
years <- c(2021:2022)
years <- sprintf('(("%d/01/01"[Date - Publication] : "%d/12/31"[Date - Publication]))', years, years)
term1 <- c('disease', 'dysbiosis') |> paste0('[Title/Abstract]')
term2 <- c('microbiome', 'microbiota') |> paste0('[Title/Abstract]')
term3 <- c('diet', 'lifestyle') |> paste0('[Title/Abstract]')
search_terms <- expand.grid("year" = years, "term1" = term1, "term2" = term2, "term3" = term3)
search_terms$final <-paste0(search_terms$year, ' AND ',
paste0('(',apply(search_terms[, grep("term.", colnames(search_terms))], 1, paste, collapse = " AND ", sep = " "),
')'
)
)
pmid_list <- lapply(search_terms$final, easyPubMed::get_pubmed_ids)
# lapply(pmid_list, "[", "Count")
# lapply(pmid_list, "[", "IdList")
pm_xml <- lapply(pmid_list, easyPubMed::fetch_pubmed_data)
pm_df_list <- lapply(pm_xml, easyPubMed::table_articles_byAuth, included_authors = "first", getKeywords = TRUE, max_chars = 500) 
pm_df_list <- Map(cbind, pm_df_list, "query" = search_terms$final, "terms" = apply(search_terms[, grep("term.", colnames(search_terms))], 1, paste, collapse = " AND ", sep = " "))
pm_df <- do.call(rbind, pm_df_list)
pm_df <- pm_df[!duplicated(pm_df), ]
```

```{r}
pm_terms_title <- litsearchr::extract_terms(text = pm_df[,"title"],
method = "fakerake", min_freq = 3, min_n = 2,
stopwords = stopwords::data_stopwords_stopwordsiso$en)
pm_terms_keywords <- litsearchr::extract_terms(keywords = trimws(unlist(strsplit(pm_df[,"keywords"], ";"))),
method = "tagged", min_freq = 3, min_n = 1, max_n = 5)
pm_terms <- c(pm_terms_title, pm_terms_keywords)
pm_terms <- pm_terms[!duplicated(pm_terms)]
```

```{r}
as.data.frame(lapply(head(pm_df[c("pmid", "doi", "jabbrv", "keywords", "abstract")]), substr, start = 1, stop = 30))
```

```{r}
pm_docs <- paste(pm_df[, "title"], pm_df[, "abstract"]) # we will consider title and abstract of each article to represent the article's "content"
pm_dfm <- litsearchr::create_dfm(elements = pm_docs, features = pm_terms) # document-feature matrix
pm_coocnet <- litsearchr::create_network(pm_dfm, min_studies = 3)
ggraph(pm_coocnet, layout = "stress") +
coord_fixed() +
expand_limits(x = c(-3, 3)) +
geom_edge_link(aes(alpha = weight)) +
geom_node_point(shape = "circle filled", fill = "white") +
geom_node_text(aes(label = name), hjust = "outward", check_overlap = TRUE) +
guides(edge_alpha = "none") +
theme_void()
```

```{r}
pm_node_strength <- igraph::strength(pm_coocnet)
pm_node_rankstrenght <- data.frame(term = names(pm_node_strength), strength = pm_node_strength, row.names = NULL)
pm_node_rankstrenght$rank <- rank(pm_node_rankstrenght$strength, ties.method = "min")
pm_node_rankstrenght <- pm_node_rankstrenght[order(pm_node_rankstrenght$rank),]
pm_plot_strenght <-
ggplot(pm_node_rankstrenght, aes(x = rank, y = strength, label = term)) +
geom_line(lwd = 0.8) +
geom_point() +
ggrepel::geom_text_repel(size = 3, hjust = "right", nudge_y = 3, max.overlaps = 30) +
theme_bw()
pm_plot_strenght
```

```{r}
pm_cutoff_cum <- litsearchr::find_cutoff(pm_coocnet, method = "cumulative", percent = 0.8)
# Changepoints - certain points along the ranking of terms where the strength of the next strongest term is much greater than that of the previous one
pm_cutoff_change <- litsearchr::find_cutoff(pm_coocnet, method = "changepoint", knot_num = 3)
pm_plot_strenght +
geom_hline(yintercept = pm_cutoff_cum, color = "red", lwd = 0.7, linetype = "longdash", alpha = 0.6) +
geom_hline(yintercept = pm_cutoff_change, color = "orange", lwd = 0.7, linetype = "dashed", alpha = 0.6)
```

```{r}
pm_cutoff_crit <- pm_cutoff_change[which.min(abs(pm_cutoff_change - pm_cutoff_cum))] # e.g. nearest cutpoint to cumulative criterion (cumulative produces one value, changepoints may be many)
pm_selected_terms <- litsearchr::get_keywords(litsearchr::reduce_graph(pm_coocnet, pm_cutoff_crit))
```

```{r}
pm_selected_terms
write.csv(pm_selected_terms, "example.csv")
```

```{r}
pm_selected_terms <- pm_selected_terms[-c(18, 19, 20, 32, 22, 25, 30)] # exclude terms
# Manual grouping into clusters - for more rigorous search we will need a combination of OR and AND operators
design <- pm_selected_terms[c(1:3, 11:15, 18, 19, 23, 25)]
intervention <- pm_selected_terms[c(4, 9, 10, 17, 21)]
disorder <- pm_selected_terms[c(5:8, 16, 20, 22, 24)]
# all.equal(length(pm_selected_terms),
# sum(length(design), length(intervention), length(disorder))
# ) # check that we grouped all terms
pm_gruped_selected_terms <- list(
design = design,
intervention = intervention,
disorder = disorder
)
```

```{r}
litsearchr::write_search(
pm_gruped_selected_terms,
languages = "English",
exactphrase = TRUE,
stemming = FALSE,
closure = "left",
writesearch = FALSE
)
```
